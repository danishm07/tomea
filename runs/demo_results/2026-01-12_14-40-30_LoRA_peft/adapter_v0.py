"""
LoRA adapter for paper
Generated by Tomea
"""

from peft import LoraConfig, get_peft_model
from transformers import AutoModelForSequenceClassification


def get_config():
    """Get LoRA configuration."""
    return LoraConfig(
        r=8,
        lora_alpha=16,
        target_modules=['query', 'key', 'value'],
        lora_dropout=0.05,
        bias="none",
        task_type="SEQ_CLS"
    )


def get_model(base_model_name: str, num_labels: int):
    """
    Apply LoRA to base model.
    
    Args:
        base_model_name: HuggingFace model name
        num_labels: Number of classification labels
    
    Returns:
        Model with LoRA applied
    """
    # Load base model
    base_model = AutoModelForSequenceClassification.from_pretrained(
        base_model_name,
        num_labels=num_labels
    )
    
    # Apply LoRA
    config = get_config()
    model = get_peft_model(base_model, config)
    
    # Print trainable parameters
    model.print_trainable_parameters()
    
    return model
